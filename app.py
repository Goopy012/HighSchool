# app.py â€” ì´ˆê°„ë‹¨ ì›¹ UI ìš”ì•½ê¸° (Streamlit 1ê°œë§Œ ì„¤ì¹˜)
# - URLì—ì„œ HTML ë‚´ë ¤ë°›ê¸°(í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬)
# - ìœ„í‚¤ ì¹œí™”í˜• <p> ë³¸ë¬¸ë§Œ ì¶”ì¶œ(HTMLParser)
# - í‚¤ì›Œë“œ(ë¹ˆë„) + 3ë¬¸ì¥ ìš”ì•½
# - í‘œ/ê°œë³„ ì¹´ë“œ + í‚¤ì›Œë“œ ë¹ˆë„ ë§‰ëŒ€ê·¸ë˜í”„ + CSV ë‹¤ìš´ë¡œë“œ

import re, io, csv
from collections import Counter
from html.parser import HTMLParser
from urllib.request import Request, urlopen

import streamlit as st

# -------------------- ì„¤ì •(ì›í•˜ë©´ ìˆ˜ì •) --------------------
DEFAULT_MAX_SENTENCES = 3
DEFAULT_TOPK = 5
MAX_SENT_CHARS = 300  # 0ì´ë©´ ë¬¸ì¥ ìë¥´ê¸° ë¹„í™œì„±í™”
STOPWORDS = set("""
ê·¸ë¦¬ê³  ê·¸ëŸ¬ë‚˜ ê·¸ë˜ì„œ ë˜ëŠ” ë˜í•œ ë° ë“± ì´ ê·¸ ì € ê²ƒ ìˆ˜ ë“±ë“± ì— ì˜ ì€ ëŠ” ì´ ê°€ ì„ ë¥¼ ìœ¼ë¡œ ë¡œ ì—ì„œ ë¶€í„° ê¹Œì§€ ë„ ë§Œ ë³´ë‹¤ ë³´ë‹¤ë„
the a an and or but if then else also to of in on at for from with by as is are was were be been being this that these those
""".split())
# -----------------------------------------------------------

def fetch_html(url: str, timeout: int = 12) -> str:
    """URLì—ì„œ HTMLì„ í…ìŠ¤íŠ¸ë¡œ ë‚´ë ¤ë°›ëŠ”ë‹¤(ê°„ë‹¨ UA/ì¸ì½”ë”© ì²˜ë¦¬)."""
    req = Request(url, headers={"User-Agent": "MiniUI/0.1 (+https://example.com)"})
    with urlopen(req, timeout=timeout) as resp:
        data = resp.read()
        ct = resp.headers.get("Content-Type", "")
        m = re.search(r"charset=([^\s;]+)", ct, re.I)
        enc = m.group(1) if m else "utf-8"
    return data.decode(enc, errors="ignore")

class POnlyParser(HTMLParser):
    """ìœ„í‚¤ ì „ìš©: ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ì•ˆì˜ <p>ë§Œ ìˆ˜ì§‘. navbox/infobox/ì°¸ê³ ë¬¸í—Œ/ëª©ì°¨/í‘œ ë“± ì œì™¸."""
    def __init__(self):
        super().__init__()
        self.in_title = False
        self.in_p = False
        self.in_content = False
        self.content_div_depth = 0
        self.skip_stack = []
        self.exclude_stack = []
        self.title_parts = []
        self.paras = []

    def _get(self, attrs, key):
        for k, v in attrs:
            if k == key: return v or ""
        return ""

    def _has_class(self, attrs, name):
        cls = self._get(attrs, "class")
        return name in (cls.split() if cls else [])

    def _has_any_class(self, attrs, names):
        cls = self._get(attrs, "class")
        if not cls: return False
        s = set(cls.split())
        return any(n in s for n in names)

    def handle_starttag(self, tag, attrs):
        # ì™„ì „ ìŠ¤í‚µ
        if tag in ("script", "style"):
            self.skip_stack.append(tag); return

        # ì œì™¸ ì˜ì—­
        excluded = False
        if tag in ("nav", "footer", "aside", "table"):
            excluded = True
        elif tag == "div":
            if (self._has_class(attrs, "navbox")
                or self._has_class(attrs, "mw-references-wrap")
                or self._has_class(attrs, "hatnote")
                or self._get(attrs, "id") in ("toc", "catlinks")):
                excluded = True
        elif tag in ("ol", "ul"):
            if self._has_class(attrs, "references"):
                excluded = True
        elif tag == "table":
            if self._has_class(attrs, "infobox") or self._has_class(attrs, "navbox"):
                excluded = True
        elif tag == "sup":
            if self._has_class(attrs, "reference"):
                excluded = True
        if excluded:
            self.exclude_stack.append(tag); return

        # ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ (div ê¹Šì´ ì¹´ìš´íŠ¸)
        if tag == "div":
            div_id = self._get(attrs, "id")
            is_content_root = (
                div_id in ("mw-content-text", "content") or
                self._has_any_class(attrs, ["mw-parser-output","mw-body","mw-body-content","content"])
            )
            if is_content_root:
                self.content_div_depth += 1
                self.in_content = True
            elif self.in_content:
                self.content_div_depth += 1

        # ì‹¤ ìˆ˜ì§‘ íƒœê·¸
        if tag == "p" and (self.in_content or self.content_div_depth == 0) and not self.exclude_stack:
            self.in_p = True
        if tag == "title":
            self.in_title = True

    def handle_endtag(self, tag):
        # ìŠ¤í‚µ ì¢…ë£Œ
        if tag in ("script", "style"):
            if self.skip_stack and self.skip_stack[-1] == tag:
                self.skip_stack.pop()
            return
        # ì œì™¸ ì˜ì—­ ì¢…ë£Œ
        if self.exclude_stack and self.exclude_stack[-1] == tag:
            self.exclude_stack.pop()
        # ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ì´íƒˆ
        if self.in_content and tag == "div":
            if self.content_div_depth > 0:
                self.content_div_depth -= 1
            if self.content_div_depth == 0:
                self.in_content = False
        # ìˆ˜ì§‘ íƒœê·¸ ì¢…ë£Œ
        if tag == "p": self.in_p = False
        if tag == "title": self.in_title = False

    def handle_data(self, data):
        if self.skip_stack or self.exclude_stack: return
        txt = (data or "").strip()
        if not txt: return
        if self.in_title:
            self.title_parts.append(txt)
        elif self.in_p and (self.in_content or self.content_div_depth == 0):
            self.paras.append(txt)

    def get_title(self): return " ".join(self.title_parts).strip()
    def get_text(self):  return " ".join(self.paras).strip()

def split_sentences(text: str):
    """ìœ„í‚¤ ê°ì£¼/ëŒ€ê´„í˜¸ ëŒ€ì‘ ë¬¸ì¥ ë¶„ë¦¬."""
    t = re.sub(r"\s+", " ", text or "").strip()
    if not t: return []
    t = re.sub(r"\[[^\]]+\]", " ", t)  # [1], [ì£¼ 2] ë“± ì œê±°
    parts = re.split(r'(?<=[.!?])(?=\s|\[)|(?<=ë‹¤\.)(?=\s|\[)', t)
    return [p.strip() for p in parts if len(p.strip()) > 2]

def tokenize(text: str):
    """ì˜ë¬¸/í•œê¸€ 2ê¸€ì ì´ìƒ í† í°ë§Œ ì¶”ì¶œ."""
    return [t.lower() for t in re.findall(r"[A-Za-zê°€-í£]{2,}", text or "")]

def keyword_topk(text: str, k: int = DEFAULT_TOPK):
    """ë¶ˆìš©ì–´ ì œì™¸ í›„ ë¹ˆë„ ìƒìœ„ kê°œ ë‹¨ì–´."""
    freq = Counter(t for t in tokenize(text) if t not in STOPWORDS)
    return [w for w,_ in freq.most_common(k)], freq

def summarize(text: str, max_sentences: int = DEFAULT_MAX_SENTENCES):
    """ë¹ˆë„ ê¸°ë°˜ ìƒìœ„ ë¬¸ì¥ ìš”ì•½."""
    sents = split_sentences(text)
    if not sents: return ""
    if len(sents) <= max_sentences:
        out = " ".join(sents); 
        return out if MAX_SENT_CHARS<=0 else (out if len(out)<=MAX_SENT_CHARS else out[:MAX_SENT_CHARS-1]+"â€¦")
    # ì „ì²´ ë¹ˆë„
    global_freq = Counter(t for t in tokenize(text) if t not in STOPWORDS)
    # ë¬¸ì¥ ì ìˆ˜
    scores = []
    for s in sents:
        score = sum(global_freq.get(t,0) for t in tokenize(s) if t not in STOPWORDS)
        scores.append(score)
    top_idx = sorted(sorted(range(len(sents)), key=lambda i: scores[i], reverse=True)[:max_sentences])
    chosen = [sents[i] for i in top_idx]
    if MAX_SENT_CHARS>0:
        chosen = [s if len(s)<=MAX_SENT_CHARS else s[:MAX_SENT_CHARS-1]+"â€¦" for s in chosen]
    return " ".join(chosen)

def process_one(url, max_sentences=DEFAULT_MAX_SENTENCES, topk=DEFAULT_TOPK):
    try:
        html = fetch_html(url)
        p = POnlyParser(); p.feed(html)
        title = p.get_title(); body = p.get_text()
        if not body:
            return {"url": url, "title": title, "keywords": "", "summary": "(ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨)", "ok": False, "freq": Counter()}
        kws, freq = keyword_topk(body, k=topk)
        summ = summarize(body, max_sentences=max_sentences)
        return {"url": url, "title": title, "keywords": ", ".join(kws), "summary": summ, "ok": True, "freq": freq}
    except Exception as e:
        return {"url": url, "title": "", "keywords": "", "summary": f"(ì—ëŸ¬: {e})", "ok": False, "freq": Counter()}

# ======================= Streamlit UI =======================
st.set_page_config(page_title="ì´ˆê°„ë‹¨ ìš”ì•½ê¸°", page_icon="ğŸ“", layout="centered")
st.title("ğŸ“ ì´ˆê°„ë‹¨ URL ìš”ì•½ê¸° (í•œ íŒŒì¼)")

st.caption("URLì„ ì¤„ë§ˆë‹¤ ì…ë ¥ â†’ ì‹¤í–‰ì„ ëˆ„ë¥´ë©´, ë¬¸ì„œë³„ **í‚¤ì›Œë“œ/3ë¬¸ì¥ ìš”ì•½**ê³¼ **í‚¤ì›Œë“œ ë¹ˆë„ ê·¸ë˜í”„**ê°€ ë³´ì—¬ìš”.")

sample = """https://ko.wikipedia.org/wiki/%EB%8C%80%ED%95%9C%EB%AF%BC%EA%B5%AD
https://ko.wikipedia.org/wiki/%EC%88%98%ED%96%89%ED%8F%89%EA%B0%80"""
urls_text = st.text_area("URL ëª©ë¡ (ì¤„ë§ˆë‹¤ 1ê°œ)", height=140, value=sample)

c1, c2, c3 = st.columns(3)
max_sent = c1.number_input("ìš”ì•½ ë¬¸ì¥ ìˆ˜", 1, 8, value=DEFAULT_MAX_SENTENCES, step=1)
topk = c2.number_input("í‚¤ì›Œë“œ ê°œìˆ˜", 3, 15, value=DEFAULT_TOPK, step=1)

run = c3.button("ì‹¤í–‰")

if run:
    urls = [u.strip() for u in urls_text.splitlines() if u.strip() and not u.strip().startswith("#")]
    if not urls:
        st.warning("URLì„ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        results = [process_one(u, max_sentences=max_sent, topk=topk) for u in urls]

        # í‘œë¡œ ì „ì²´ ê²°ê³¼ ë³´ê¸°
        table_rows = [{"url": r["url"], "title": r["title"], "keywords": r["keywords"], "summary": r["summary"]} for r in results]
        st.subheader("ğŸ“‹ ê²°ê³¼ í‘œ")
        st.dataframe(table_rows, use_container_width=True)

        # ê°œë³„ ì¹´ë“œ
        st.subheader("ğŸ§¾ ë¬¸ì„œë³„ ìš”ì•½")
        for r in results:
            with st.expander(r["title"] or r["url"], expanded=False):
                st.markdown(f"**URL:** {r['url']}")
                st.markdown(f"**í‚¤ì›Œë“œ:** {r['keywords'] or '(ì—†ìŒ)'}")
                st.markdown(f"**ìš”ì•½({max_sent}ë¬¸ì¥):** {r['summary']}")

        # í‚¤ì›Œë“œ ì „ì²´ ë¹ˆë„ ì‹œê°í™”
        st.subheader("ğŸ“Š ì „ì²´ í‚¤ì›Œë“œ ë¹ˆë„ (ìƒìœ„ 15)")
        total = Counter()
        for r in results:
            total.update(r["freq"])
        # ë¶ˆìš©ì–´ ì œê±°ëœ ì „ì²´ ë¶„í¬ì—ì„œ ìƒìœ„ 15ê°œë§Œ
        top_items = total.most_common(15)
        if top_items:
            import pandas as pd
            df = pd.DataFrame(top_items, columns=["keyword", "count"]).set_index("keyword")
            st.bar_chart(df)  # ê°„ë‹¨ ë§‰ëŒ€ê·¸ë˜í”„
        else:
            st.info("í‘œì‹œí•  í‚¤ì›Œë“œê°€ ì—†ì–´ìš”.")

        # CSV ë‹¤ìš´ë¡œë“œ
        buf = io.StringIO()
        w = csv.DictWriter(buf, fieldnames=["url", "title", "keywords", "summary"])
        w.writeheader(); w.writerows(table_rows)
        st.download_button("CSV ë‹¤ìš´ë¡œë“œ", data=buf.getvalue().encode("utf-8-sig"), file_name="results.csv", mime="text/csv")
